{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "hCQ3yt7Ywg0R",
        "28ePEm2TzDLX",
        "Zd2flwegzW1A",
        "4K3BYEqzzcPo",
        "bILQIEpPzmy4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTwlwBiR21Ky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execution time\n",
        "import time\n",
        "start = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AMTnmq1v-7S",
        "colab_type": "text"
      },
      "source": [
        "# Google Colab Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc60ZxQ6GdDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn.svm as svm\n",
        "import sklearn.tree as tree\n",
        "import sklearn.ensemble as ensemble\n",
        "import sklearn.neighbors as neighbors\n",
        "import sklearn.naive_bayes as naive_bayes\n",
        "import sklearn.linear_model as linear_model\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn import preprocessing as preproc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error, mean_absolute_error, roc_curve, auc\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_vMpcWR6fUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accessing Google sheets\n",
        "!pip install --upgrade -q gspread\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "worksheet = gc.open('AutoKaggle').worksheet('Metadata')\n",
        "\n",
        "# get_all_values gives a list of rows\n",
        "_rows = worksheet.get_all_values()\n",
        "\n",
        "# Convert to a DataFrame and render.\n",
        "import pandas as pd\n",
        "rows = pd.DataFrame.from_records(_rows)\n",
        "\n",
        "new_header = rows.iloc[0] #grab the first row for the header\n",
        "rows = rows[1:] #take the data less the header row\n",
        "rows.columns = new_header #set the header row as the df header"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shhl7NnQ6gaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def alpha_to_number(alpha_key):\n",
        "  return sum([(ord(alpha)-64)*(26**ind) for ind, alpha in enumerate(list(alpha_key)[::-1])]) - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxFQTU7wwH6z",
        "colab_type": "text"
      },
      "source": [
        "# Mount at Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOwYBjH5wMuK",
        "colab_type": "text"
      },
      "source": [
        "If cannot read from the file,  please rerun this statement until \"gdrive/My Drive\" appears on the left bar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJG0sKlxtaSW",
        "colab_type": "code",
        "outputId": "ec31cc27-db7a-4b17-903e-55e741068102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCQ3yt7Ywg0R",
        "colab_type": "text"
      },
      "source": [
        "# Metadata Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWeydbYR6oZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parseMetaData(row_id):\n",
        "  \n",
        "  # Parse data from MetaData for each row\n",
        "  column_key = {'name': 'C', 'columns': 'W', 'estimator_func_call': 'AU', 'target_name': 'AC', 'output_type': 'AA', 'performance_metric': 'BB', 'feature_selector': 'AL'}\n",
        "  column_key = dict(map(lambda kv: (kv[0], alpha_to_number(kv[1])), column_key.items()))\n",
        "  \n",
        "  metadata['competition_name'] = rows.loc[row_id][column_key['name']]\n",
        "  metadata['estimator'] = rows.loc[row_id][column_key['estimator_func_call']]\n",
        "  metadata['target_column'] = rows.loc[row_id][column_key['target_name']]\n",
        "  metadata['output_type'] = rows.loc[row_id][column_key['output_type']].split(',')\n",
        "  metadata['metric'] = rows.loc[row_id][column_key['performance_metric']]\n",
        "  metadata['feature_selector'] = rows.loc[row_id][column_key['feature_selector']]\n",
        "  columns = rows.loc[row_id][column_key['columns']]\n",
        "\n",
        "  # Parse column information \n",
        "  numeric_columns = []\n",
        "  unwanted_columns = []\n",
        "  categorical_columns = []\n",
        "  columns_data = [x.strip() for x in columns[1:-1].split(';')]\n",
        "  for ind, val in enumerate(columns_data):\n",
        "    if ind%3 == 2:\n",
        "      if (val == \"numeric\" or val == \"integer\" or val == \"real\"):\n",
        "        numeric_columns.append(columns_data[ind-1])\n",
        "      elif val == \"categorical\":\n",
        "        categorical_columns.append(columns_data[ind-1])\n",
        "      elif val == \"unwanted\" or val == \"string\" or val == 'dateTime':\n",
        "        unwanted_columns.append(columns_data[ind-1])\n",
        "    else:\n",
        "      pass\n",
        "    \n",
        "  metadata['numeric_columns'] = numeric_columns\n",
        "  metadata['unwanted_columns'] = unwanted_columns\n",
        "  metadata['categorical_columns'] = categorical_columns\n",
        "  \n",
        "  # Remove target from features columns\n",
        "  if metadata['target_column'] in metadata['numeric_columns']:\n",
        "    metadata['numeric_columns'].remove(metadata['target_column'])\n",
        "  if metadata['target_column'] in metadata['categorical_columns']:\n",
        "    metadata['categorical_columns'].remove(metadata['target_column'])\n",
        "  if metadata['target_column'] in metadata['unwanted_columns']:\n",
        "    metadata['unwanted_columns'].remove(metadata['target_column'])\n",
        "  \n",
        "  print(metadata['competition_name'])\n",
        "  print(metadata['numeric_columns'])\n",
        "  print(metadata['categorical_columns'])\n",
        "  print(metadata['unwanted_columns'])\n",
        "  print(metadata['target_column'])\n",
        "  print(metadata['metric'])\n",
        "  print(metadata['feature_selector'])\n",
        "  print(metadata['estimator'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28ePEm2TzDLX",
        "colab_type": "text"
      },
      "source": [
        "# Add relevent import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rRxxUSsjj_D",
        "colab_type": "code",
        "outputId": "840fd144-bdde-43dd-dd07-d16890e17bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Installations\n",
        "!pip install --upgrade -q gspread\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import gspread\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "import random\n",
        "from math import exp\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Imports\n",
        "# Preprocessing imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import sklearn.ensemble as ensemble\n",
        "from sklearn import preprocessing as preproc\n",
        "from sklearn import preprocessing\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feature extraction imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Feature selection imports\n",
        "\n",
        "# Estimation imports\n",
        "from sklearn.metrics import accuracy_score, log_loss,mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn import model_selection, preprocessing, linear_model\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "from keras.utils.np_utils import to_categorical \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "import scipy\n",
        "import re\n",
        "\n",
        "# Other initializations\n",
        "sns.set(style='white', context='notebook', palette='deep')\n",
        "epochs_completed = 0\n",
        "index_in_epoch = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ridZv_rCF6h4",
        "colab_type": "code",
        "outputId": "7c60d703-9240-48bc-c707-1251292c531c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4WBvMQ6zOLg",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MMk3DPzDx-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(train_df, test_df):\n",
        "  y_test_age=0\n",
        "  y_train_age=0\n",
        "  test=0\n",
        "  test_data=0\n",
        "  \n",
        "  if train_df.shape[0]==42000:           #digit\n",
        "    # drop target columns\n",
        "    X = train_df.drop(labels = [\"label\"],axis = 1)\n",
        "    y = train_df[\"label\"] \n",
        "    X = X / 255.0\n",
        "    test_df = test_df / 255.0  \n",
        "    # Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
        "    X = X.values.reshape(-1,28,28,1)\n",
        "    test = test_df.values.reshape(-1,28,28,1)  \n",
        "    # Encode labels to one hot vectors\n",
        "    y = to_categorical(y, num_classes = (np.max(y)+1))\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=2)\n",
        "    \n",
        "  if train_df.shape[0]==637:              #delete\n",
        "    # drop target columns\n",
        "    X = train_df.drop(labels = [\"label\"],axis = 1)\n",
        "    y = train_df[\"label\"] \n",
        "    X = train_df.iloc[:,1:].values\n",
        "    test = test_df.iloc[:,1:].values\n",
        "    X = X/255.0\n",
        "    test = test/255.0\n",
        "    y = to_categorical(y, num_classes =(np.max(y)+1))\n",
        "    # split data into training & validation\n",
        "    X_test = X[:80]\n",
        "    y_test = y[:80]\n",
        "    X_train = X[80:]\n",
        "    y_train = y[80:]\n",
        "    \n",
        "  if train_df.shape[0]==2400:\n",
        "    # Degree to radian\n",
        "    train_df['alpha_rad'] = np.radians(train_df['lattice_angle_alpha_degree'])\n",
        "    train_df['beta_rad'] = np.radians(train_df['lattice_angle_beta_degree'])\n",
        "    train_df['gamma_rad'] = np.radians(train_df['lattice_angle_gamma_degree'])\n",
        "    test_df['alpha_rad'] = np.radians(test_df['lattice_angle_alpha_degree'])\n",
        "    test_df['beta_rad'] = np.radians(test_df['lattice_angle_beta_degree'])\n",
        "    test_df['gamma_rad'] = np.radians(test_df['lattice_angle_gamma_degree'])\n",
        "    def vol(df):\n",
        "        volumn = df['lattice_vector_1_ang']*df['lattice_vector_2_ang']*df['lattice_vector_3_ang']*np.sqrt(\n",
        "        1 + 2*np.cos(df['alpha_rad'])*np.cos(df['beta_rad'])*np.cos(df['gamma_rad'])\n",
        "        -np.cos(df['alpha_rad'])**2-np.cos(df['beta_rad'])**2-np.cos(df['gamma_rad'])**2)\n",
        "        df['volumn'] = volumn\n",
        "    vol(train_df)\n",
        "    vol(test_df)\n",
        "    # Atomic density\n",
        "    train_df['density'] = train_df['number_of_total_atoms'] / train_df['volumn']\n",
        "    test_df['density'] = test_df['number_of_total_atoms'] / test_df['volumn']\n",
        "    col = ['formation_energy_ev_natom','bandgap_energy_ev']\n",
        "    X_train = train_df.drop(['id']+col,axis=1)\n",
        "    y_train = train_df[col]\n",
        "    X_test = test_df.drop(['id'],axis=1)\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(X_train)\n",
        "    X_test= sc.fit_transform(X_test)\n",
        "    y_train = y_train.values\n",
        "    y_test = 0\n",
        "    \n",
        "  if train_df.shape[0]==19579:                  #author\n",
        "    from sklearn import preprocessing\n",
        "    test_data = test_df.loc[:,'text'].reset_index(drop=True)\n",
        "    stratifiedCV = model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=1)\n",
        "    trainInds, validInds = next(stratifiedCV.split(train_df['text'], train_df['author']))\n",
        "    X_train = train_df.loc[trainInds,'text'].reset_index(drop=True)\n",
        "    X_test  = train_df.loc[validInds,'text'].reset_index(drop=True)\n",
        "    trainLabel = train_df.loc[trainInds,'author'].reset_index(drop=True)\n",
        "    validLabel = train_df.loc[validInds,'author'].reset_index(drop=True)\n",
        "    yLabelEncoder = preprocessing.LabelEncoder()\n",
        "    yLabelEncoder.fit(pd.concat((trainLabel,validLabel)))\n",
        "    y_train = yLabelEncoder.transform(trainLabel)\n",
        "    y_test = yLabelEncoder.transform(validLabel)\n",
        "    \n",
        "  if train_df.shape[0]==7395:                    #stumbleupon,137\n",
        "    X_train = list(np.array(train_df)[:,2])\n",
        "    X_test = list(np.array(test_df)[:,2])\n",
        "    y_train = np.array(train_df)[:,-1]\n",
        "    y_train = y_train.astype('int')\n",
        "    y_test = 0\n",
        "    \n",
        "  if train_df.shape[0]==49352:\n",
        "    X_train=train_df\n",
        "    X_test=test_df\n",
        "    interest_level_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "    X_train['interest_level'] = X_train['interest_level'].apply(lambda x: interest_level_map[x])\n",
        "    X_test['interest_level'] = -1\n",
        "    X_train['price'].ix[X_train['price']>13000] = 13000\n",
        "    #add features\n",
        "    feature_transform = CountVectorizer(stop_words='english', max_features=150)\n",
        "    X_train['features'] = X_train[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\n",
        "    X_test['features'] = X_test[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\n",
        "    feature_transform.fit(list(X_train['features']) + list(X_test['features']))\n",
        "    train_size = len(X_train)\n",
        "    low_count = len(X_train[X_train['interest_level'] == 0])\n",
        "    medium_count = len(X_train[X_train['interest_level'] == 1])\n",
        "    high_count = len(X_train[X_train['interest_level'] == 2])\n",
        "\n",
        "    def find_objects_with_only_one_record(feature_name):\n",
        "        temp = pd.concat([X_train[feature_name].reset_index(), \n",
        "                          X_test[feature_name].reset_index()])\n",
        "        temp = temp.groupby(feature_name, as_index = False).count()\n",
        "        return temp[temp['index'] == 1]\n",
        "    managers_with_one_lot = find_objects_with_only_one_record('manager_id')\n",
        "    buildings_with_one_lot = find_objects_with_only_one_record('building_id')\n",
        "    addresses_with_one_lot = find_objects_with_only_one_record('display_address')\n",
        "    lambda_val = None\n",
        "    k=5.0\n",
        "    f=1.0\n",
        "    r_k=0.01 \n",
        "    g = 1.0\n",
        "    def categorical_average(variable, y, pred_0, feature_name):\n",
        "        def calculate_average(sub1, sub2):\n",
        "            s = pd.DataFrame(data = {\n",
        "                                     variable: sub1.groupby(variable, as_index = False).count()[variable],                              \n",
        "                                     'sumy': sub1.groupby(variable, as_index = False).sum()['y'],\n",
        "                                     'avgY': sub1.groupby(variable, as_index = False).mean()['y'],\n",
        "                                     'cnt': sub1.groupby(variable, as_index = False).count()['y']\n",
        "                                     })\n",
        "            tmp = sub2.merge(s.reset_index(), how='left', left_on=variable, right_on=variable) \n",
        "            del tmp['index']                       \n",
        "            tmp.loc[pd.isnull(tmp['cnt']), 'cnt'] = 0.0\n",
        "            tmp.loc[pd.isnull(tmp['cnt']), 'sumy'] = 0.0\n",
        "            def compute_beta(row):\n",
        "                cnt = row['cnt'] if row['cnt'] < 200 else float('inf')\n",
        "                return 1.0 / (g + exp((cnt - k) / f))\n",
        "            if lambda_val is not None:\n",
        "                tmp['beta'] = lambda_val\n",
        "            else:\n",
        "                tmp['beta'] = tmp.apply(compute_beta, axis = 1)\n",
        "            tmp['adj_avg'] = tmp.apply(lambda row: (1.0 - row['beta']) * row['avgY'] + row['beta'] * row['pred_0'],\n",
        "                                       axis = 1)\n",
        "            tmp.loc[pd.isnull(tmp['avgY']), 'avgY'] = tmp.loc[pd.isnull(tmp['avgY']), 'pred_0']\n",
        "            tmp.loc[pd.isnull(tmp['adj_avg']), 'adj_avg'] = tmp.loc[pd.isnull(tmp['adj_avg']), 'pred_0']\n",
        "            tmp['random'] = np.random.uniform(size = len(tmp))\n",
        "            tmp['adj_avg'] = tmp.apply(lambda row: row['adj_avg'] *(1 + (row['random'] - 0.5) * r_k),\n",
        "                                       axis = 1)\n",
        "            return tmp['adj_avg'].ravel()\n",
        "        #cv for training set \n",
        "        k_fold = StratifiedKFold(5)\n",
        "        X_train[feature_name] = -999 \n",
        "        for (train_index, cv_index) in k_fold.split(np.zeros(len(X_train)),\n",
        "                                                    X_train['interest_level'].ravel()):\n",
        "            sub = pd.DataFrame(data = {variable: X_train[variable],\n",
        "                                       'y': X_train[y],\n",
        "                                       'pred_0': X_train[pred_0]})\n",
        "            sub1 = sub.iloc[train_index]        \n",
        "            sub2 = sub.iloc[cv_index]\n",
        "            X_train.loc[cv_index, feature_name] = calculate_average(sub1, sub2)\n",
        "        #for test set\n",
        "        sub1 = pd.DataFrame(data = {variable: X_train[variable],\n",
        "                                    'y': X_train[y],\n",
        "                                    'pred_0': X_train[pred_0]})\n",
        "        sub2 = pd.DataFrame(data = {variable: X_test[variable],\n",
        "                                    'y': X_test[y],\n",
        "                                    'pred_0': X_test[pred_0]})\n",
        "        X_test.loc[:, feature_name] = calculate_average(sub1, sub2)                               \n",
        "    def transform_data(X):\n",
        "        #add features    \n",
        "        feat_sparse = feature_transform.transform(X[\"features\"])\n",
        "        vocabulary = feature_transform.vocabulary_\n",
        "        del X['features']\n",
        "        X1 = pd.DataFrame([ pd.Series(feat_sparse[i].toarray().ravel()) for i in np.arange(feat_sparse.shape[0]) ])\n",
        "        X1.columns = list(sorted(vocabulary.keys()))\n",
        "        X = pd.concat([X.reset_index(), X1.reset_index()], axis = 1)\n",
        "        del X['index']\n",
        "        X[\"num_photos\"] = X[\"photos\"].apply(len)\n",
        "        X['created'] = pd.to_datetime(X[\"created\"])\n",
        "        X[\"num_description_words\"] = X[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "        X['price_per_bed'] = X['price'] / X['bedrooms']    \n",
        "        X['price_per_bath'] = X['price'] / X['bathrooms']\n",
        "        X['price_per_room'] = X['price'] / (X['bathrooms'] + X['bedrooms'] )\n",
        "        X['low'] = 0\n",
        "        X.loc[X['interest_level'] == 0, 'low'] = 1\n",
        "        X['medium'] = 0\n",
        "        X.loc[X['interest_level'] == 1, 'medium'] = 1\n",
        "        X['high'] = 0\n",
        "        X.loc[X['interest_level'] == 2, 'high'] = 1\n",
        "        X['display_address'] = X['display_address'].apply(lambda x: x.lower().strip())\n",
        "        X['street_address'] = X['street_address'].apply(lambda x: x.lower().strip())\n",
        "        X['pred0_low'] = low_count * 1.0 / train_size\n",
        "        X['pred0_medium'] = medium_count * 1.0 / train_size\n",
        "        X['pred0_high'] = high_count * 1.0 / train_size\n",
        "        X.loc[X['manager_id'].isin(managers_with_one_lot['manager_id'].ravel()), \n",
        "              'manager_id'] = \"-1\"\n",
        "        X.loc[X['building_id'].isin(buildings_with_one_lot['building_id'].ravel()), \n",
        "              'building_id'] = \"-1\"\n",
        "        X.loc[X['display_address'].isin(addresses_with_one_lot['display_address'].ravel()), \n",
        "              'display_address'] = \"-1\"\n",
        "        return X\n",
        "    def normalize_high_cordiality_data():\n",
        "        high_cardinality = [\"building_id\", \"manager_id\"]\n",
        "        for c in high_cardinality:\n",
        "            categorical_average(c, \"medium\", \"pred0_medium\", c + \"_mean_medium\")\n",
        "            categorical_average(c, \"high\", \"pred0_high\", c + \"_mean_high\")\n",
        "    def transform_categorical_data():\n",
        "        categorical = ['building_id', 'manager_id', \n",
        "                       'display_address', 'street_address']\n",
        "        for f in categorical:\n",
        "            encoder = LabelEncoder()\n",
        "            encoder.fit(list(X_train[f]) + list(X_test[f])) \n",
        "            X_train[f] = encoder.transform(X_train[f].ravel())\n",
        "            X_test[f] = encoder.transform(X_test[f].ravel())\n",
        "    def remove_columns(X):\n",
        "        columns = [\"photos\", \"pred0_high\", \"pred0_low\", \"pred0_medium\",\n",
        "                   \"description\", \"low\", \"medium\", \"high\",\n",
        "                   \"interest_level\", \"created\"]\n",
        "        for c in columns:\n",
        "            del X[c]\n",
        "    print(\"Starting transformations\")        \n",
        "    X_train= transform_data(X_train)    \n",
        "    X_test = transform_data(X_test) \n",
        "    y_train = X_train['interest_level'].ravel()\n",
        "    print(\"Normalizing high cordiality data...\")\n",
        "    normalize_high_cordiality_data()\n",
        "    transform_categorical_data()\n",
        "    remove_columns(X_train)\n",
        "    remove_columns(X_test)\n",
        "    y_test = 0\n",
        "    \n",
        "  return X_train, X_test, y_train, y_test, y_test_age, y_train_age, test, test_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd2flwegzW1A",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlEz3TgMD2bL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_extraction(X_train, X_test, y_train, y_test):\n",
        "  \n",
        "  if y_train.shape[0]==7395:\n",
        "    tfv = TfidfVectorizer(min_df=3,  max_features=None, strip_accents='unicode',  \n",
        "        analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
        "    X_all = X_train + X_test\n",
        "    tfv.fit(X_all)\n",
        "    X_train_f = tfv.transform(X_train)\n",
        "    X_test_f = tfv.transform(X_test)\n",
        "  else:\n",
        "    X_train_f = 0 \n",
        "    X_test_f = 0\n",
        "\n",
        "  return X_train_f, X_test_f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K3BYEqzzcPo",
        "colab_type": "text"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOtDPzvCD5YL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_selection():\n",
        "  pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t-lM6gfzhCx",
        "colab_type": "text"
      },
      "source": [
        "# Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U32siwMD8WF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def estimation(X_train, X_test, y_train, y_test, test, test_data, X_train_featured, X_test_featured, \n",
        "               y_test_age, y_train_age): \n",
        "  \n",
        "  ##################### Tensorflow&CNN: letseat ######################\n",
        "  if y_train.shape[0] == 557:\n",
        "    \n",
        "    LEARNING_RATE = 1e-4 \n",
        "    TRAINING_ITERATIONS = 1000        \n",
        "    DROPOUT = 0.5\n",
        "    BATCH_SIZE = 40\n",
        "    IMAGE_DIMENSION_SIZE = 28\n",
        "    def weight_variable(shape):\n",
        "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "        return tf.Variable(initial)\n",
        "    def bias_variable(shape):\n",
        "        initial = tf.constant(0.1, shape=shape)\n",
        "        return tf.Variable(initial)\n",
        "    def conv2d(x, W):\n",
        "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "    def max_pool_2x2(x):\n",
        "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "    x = tf.placeholder('float', shape=[None, 784])\n",
        "    y_ = tf.placeholder('float', shape=[None, 3])\n",
        "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
        "    b_conv1 = bias_variable([32])\n",
        "    image = tf.reshape(x, [-1,28,28,1])\n",
        "    h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
        "    h_pool1 = max_pool_2x2(h_conv1)\n",
        "    layer1 = tf.reshape(h_conv1, (-1, 28, 28, 4 ,8))  \n",
        "    layer1 = tf.transpose(layer1, (0, 3, 1, 4,2))\n",
        "    layer1 = tf.reshape(layer1, (-1, 28*4, 28*8)) \n",
        "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
        "    b_conv2 = bias_variable([64])\n",
        "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "    h_pool2 = max_pool_2x2(h_conv2)\n",
        "    layer2 = tf.reshape(h_conv2, (-1, 14, 14, 4 ,16))  \n",
        "    layer2 = tf.transpose(layer2, (0, 3, 1, 4,2))\n",
        "    layer2 = tf.reshape(layer2, (-1, 14*4, 14*16)) \n",
        "    W_fc1 = weight_variable([IMAGE_DIMENSION_SIZE*IMAGE_DIMENSION_SIZE*4, 1024])\n",
        "    b_fc1 = bias_variable([1024])\n",
        "    h_pool2_flat = tf.reshape(h_pool2, [-1, IMAGE_DIMENSION_SIZE*IMAGE_DIMENSION_SIZE*4])\n",
        "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "    keep_prob = tf.placeholder('float')\n",
        "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "    W_fc2 = weight_variable([1024, 3])\n",
        "    b_fc2 = bias_variable([3])\n",
        "    y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
        "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
        "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
        "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
        "    predict = tf.argmax(y,1)    \n",
        "    num_examples = X_train.shape[0]\n",
        "    def next_batch(batch_size):\n",
        "        global X_train\n",
        "        global y_train\n",
        "        global index_in_epoch\n",
        "        global epochs_completed\n",
        "        start = index_in_epoch\n",
        "        index_in_epoch += batch_size    \n",
        "        if index_in_epoch > num_examples:\n",
        "            epochs_completed += 1\n",
        "            perm = np.arange(num_examples)\n",
        "            np.random.shuffle(perm)\n",
        "            X_train = X_train[perm]\n",
        "            y_train = y_train[perm]\n",
        "            start = 0\n",
        "            index_in_epoch = batch_size\n",
        "            assert batch_size <= num_examples\n",
        "        end = index_in_epoch\n",
        "        return X_train[start:end], y_train[start:end]\n",
        "    init = tf.initialize_all_variables()\n",
        "    sess = tf.InteractiveSession()\n",
        "    sess.run(init)\n",
        "    train_accuracies = []\n",
        "    validation_accuracies = []\n",
        "    x_range = []\n",
        "    display_step=1\n",
        "    for i in range(TRAINING_ITERATIONS):\n",
        "        batch_xs, batch_ys = next_batch(BATCH_SIZE)        \n",
        "        if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
        "            train_accuracy = accuracy.eval(feed_dict={x:batch_xs, y_: batch_ys, keep_prob: 1.0})       \n",
        "            if(80):\n",
        "                validation_accuracy = accuracy.eval(feed_dict={ x: X_test[0:BATCH_SIZE],y_: y_test[0:BATCH_SIZE], keep_prob: 1.0})                                  \n",
        "                print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
        "                validation_accuracies.append(validation_accuracy)\n",
        "            else:\n",
        "                 print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            x_range.append(i)\n",
        "            if i%(display_step*10) == 0 and i:\n",
        "                display_step *= 10\n",
        "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT}) \n",
        "    if(80):\n",
        "        validation_accuracy = accuracy.eval(feed_dict={x: X_test,y_: y_test,keep_prob: 1.0})\n",
        "        print('validation_accuracy => %.4f'%validation_accuracy)\n",
        "    sess.close()\n",
        "    \n",
        "  ##################### Tensorflow&CNN: letseat ######################\n",
        "  \n",
        "  ######################### Keras&CNN: digit ######################### \n",
        "  if y_train.shape[0]==37800:\n",
        "    \n",
        "    #CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))\n",
        "    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\n",
        "    model.add(MaxPool2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation = \"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation = \"softmax\"))\n",
        "    # Define the optimizer\n",
        "    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "    # Compile the model\n",
        "    model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    # Set a learning rate annealer\n",
        "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)   \n",
        "    epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
        "    batch_size = 86\n",
        "    # With data augmentation to prevent overfitting (accuracy 0.99286)\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "    datagen.fit(X_train)\n",
        "    # Fit the model\n",
        "    history = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (X_test,y_test),\n",
        "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
        "                              , callbacks=[learning_rate_reduction])\n",
        "    results = model.predict(test)\n",
        "    results = np.argmax(results,axis = 1)\n",
        "    results = pd.Series(results,name=\"Label\")\n",
        "    submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
        "\n",
        "  ######################### Keras&CNN: digit ######################### \n",
        "  \n",
        "  ####################### Keras&ANN: Conductor #######################\n",
        "  if y_train.shape[0]==2400:\n",
        "    def rmsle(y_true,y_pred):\n",
        "      return np.sqrt(np.square(np.log(y_pred + 1) - np.log(y_true + 1)).mean()) \n",
        "    regressor = Sequential()\n",
        "    #1 and hidden layer\n",
        "    regressor.add(Dense(units = 1024, activation = 'relu', kernel_initializer = 'glorot_uniform',input_dim = X_train.shape[1]))\n",
        "    regressor.add(Dropout(0.1))\n",
        "    regressor.add(Dense(units = 512, activation = 'relu', kernel_initializer = 'uniform'))\n",
        "    regressor.add(Dropout(0.1))\n",
        "    regressor.add(Dense(units = 64, activation = 'relu', kernel_initializer = 'uniform'))\n",
        "    regressor.add(Dropout(0.1))\n",
        "    regressor.add(Dense(units = 2, activation = 'relu', kernel_initializer = 'uniform'))\n",
        "    #compile ANN\n",
        "    regressor.compile(optimizer = 'adam', loss = 'mse', metrics =['accuracy'])\n",
        "    regressor.fit(X_train,y_train,batch_size = 3, epochs = 50, validation_split=0.1)\n",
        "    #Local CV\n",
        "    rmsle(y_train,regressor.predict(X_train))\n",
        "    rmsle = rmsle(y_train,regressor.predict(X_train))\n",
        "    print(rmsle)\n",
        "  ####################### Keras&ANN: Conductor #######################\n",
        "  \n",
        "  ################### Logistic Regression: Author ####################\n",
        "  if y_train.shape[0]==17621:\n",
        "    from sklearn import preprocessing\n",
        "    ngramLength = 5\n",
        "    print('-'*52)\n",
        "    print('fitting \"CountVectorizer()\" for bag of char %d-grams' %(ngramLength))\n",
        "    BagOfCharsExtractor = CountVectorizer(min_df=8, max_features=250000, \n",
        "                                          analyzer='char', ngram_range=(1,ngramLength), \n",
        "                                          binary=False,lowercase=True)\n",
        "    BagOfCharsExtractor.fit(pd.concat((X_train,X_test,test_data)))\n",
        "    X_train_char = BagOfCharsExtractor.transform(X_train)\n",
        "    X_valid_char = BagOfCharsExtractor.transform(X_test)\n",
        "    X_test_char  = BagOfCharsExtractor.transform(test_data)\n",
        "    ngramLength = 2\n",
        "    print('-'*52)\n",
        "    print('fitting \"CountVectorizer()\" for bag of word %d-grams' %(ngramLength))\n",
        "    BagOfWordsExtractor = CountVectorizer(min_df=5, max_features=250000, \n",
        "                                          analyzer='word', ngram_range=(1,ngramLength), \n",
        "                                          binary=False,lowercase=True)\n",
        "    BagOfWordsExtractor.fit(pd.concat((X_train,X_test,test_data)))\n",
        "    X_train_word = BagOfWordsExtractor.transform(X_train)\n",
        "    X_valid_word = BagOfWordsExtractor.transform(X_test)\n",
        "    X_test_word  = BagOfWordsExtractor.transform(test_data)\n",
        "    # combine and scale features \n",
        "    X_train = scipy.sparse.hstack((X_train_word,X_train_char))\n",
        "    X_test = scipy.sparse.hstack((X_valid_word,X_valid_char))\n",
        "    test_data  = scipy.sparse.hstack((X_test_word,X_test_char))\n",
        "    stdScaler = preprocessing.StandardScaler(with_mean=False)\n",
        "    stdScaler.fit(scipy.sparse.vstack(((X_train,X_test,test_data))))\n",
        "\n",
        "    X_train = stdScaler.transform(X_train)\n",
        "    X_test = stdScaler.transform(X_test)\n",
        "    print('fitting \"LogisticRegression()\" classifier')\n",
        "    logisticRegressor = linear_model.LogisticRegression(C=0.01, solver='sag')\n",
        "    logisticRegressor.fit(X_train, y_train)\n",
        "    trainAccuracy = accuracy_score(y_train, logisticRegressor.predict(X_train))\n",
        "    trainLogLoss = log_loss(y_train, logisticRegressor.predict_proba(X_train))\n",
        "    validAccuracy = accuracy_score(y_test, logisticRegressor.predict(X_test))\n",
        "    validLogLoss = log_loss(y_test, logisticRegressor.predict_proba(X_test))\n",
        "    print('Train: %.1f%s Accuracy, log loss = %.4f' % (100*trainAccuracy,'%',trainLogLoss))\n",
        "    print('Valid: %.1f%s Accuracy, log loss = %.4f' % (100*validAccuracy,'%',validLogLoss))\n",
        "  ################## Logistic Regression: Author ####################\n",
        "  \n",
        "  ############### Logistic Regression: Stumbleupon ##################\n",
        "  if y_train.shape[0]==7395:\n",
        "    import sklearn.linear_model as lm\n",
        "    rd = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, \n",
        "                             C=1, fit_intercept=True, intercept_scaling=1.0, \n",
        "                             class_weight=None, random_state=None)\n",
        "    print (\"20 Fold CV Score: \", np.mean(model_selection.cross_val_score(rd, X_train_featured, y_train, cv=20, scoring='roc_auc')))\n",
        "  ############### Logistic Regression: Stumbleupon ##################\n",
        "  \n",
        "  ########################## Xgboost: Sigma #########################\n",
        "  if y_train.shape[0]==49352:\n",
        "    param = {}\n",
        "    param['objective'] = 'multi:softprob'\n",
        "    param['eta'] = 0.02\n",
        "    param['max_depth'] = 6\n",
        "    param['silent'] = 1\n",
        "    param['num_class'] = 3\n",
        "    param['eval_metric'] = \"mlogloss\"\n",
        "    param['min_child_weight'] = 3\n",
        "    param['subsample'] = 0.7\n",
        "    param['colsample_bytree'] = 0.7\n",
        "    param['seed'] = 321\n",
        "    param['nthread'] = 8\n",
        "    param['verbose'] = 1\n",
        "    param['print_every_n'] = 1\n",
        "    num_rounds = 300\n",
        "    xgtrain = xgb.DMatrix(X_train , label=y_train)\n",
        "    watchlist = [(xgtrain, 'train')]\n",
        "    clf = xgb.train(param, xgtrain, num_rounds, watchlist)\n",
        "    print(\"Fitted\")\n",
        "  ########################## Xgboost: Sigma #########################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bILQIEpPzmy4",
        "colab_type": "text"
      },
      "source": [
        "# Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SA4RsM5EAG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def postprocessing():\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VstUncpAzsDv",
        "colab_type": "text"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRjIN5snGRN6",
        "colab_type": "text"
      },
      "source": [
        "Please refer to different training and testing dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMfNxyyyECzK",
        "colab_type": "code",
        "outputId": "a1e0f9bd-58bd-4eb4-d51e-25cf258e41a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9603
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "row_ids = [3, 228, 379, 335, 137, 504]\n",
        "# row_ids = [504]\n",
        "metadata={}\n",
        "\n",
        "#Set current working directory\n",
        "cwd = 'gdrive/My Drive/Introduction to Data Science Spring 2019 Term Project/jy2823_yz4953/'\n",
        "\n",
        "for row_id in row_ids:\n",
        "  metadata.clear()\n",
        "  print(\"************************************************************\")  \n",
        "  parseMetaData(row_id)\n",
        "  \n",
        "  if row_id == 3:\n",
        "    train_dir = cwd + metadata['competition_name'] + '/data/train.csv' \n",
        "    test_dir = cwd + metadata['competition_name'] + '/data/test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "  if row_id == 228:\n",
        "    train_dir = cwd + metadata['competition_name'] + '/data/utensils_train.csv' \n",
        "    test_dir = cwd + metadata['competition_name'] + '/data/utensils_test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "  if row_id == 379:\n",
        "    train_dir = cwd + metadata['competition_name'] + '/data/train.csv' \n",
        "    test_dir = cwd + metadata['competition_name'] + '/data/test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "  if row_id == 335:\n",
        "    train_dir = cwd + metadata['competition_name'] + '/data/train.csv' \n",
        "    test_dir = cwd + metadata['competition_name'] + '/data/test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "  if row_id == 137:\n",
        "    train_dir = cwd + metadata['competition_name'] + '/data/train.tsv' \n",
        "    test_dir = cwd + metadata['competition_name'] + '/data/test.tsv'\n",
        "    train_df = pd.read_table(train_dir)\n",
        "    test_df = pd.read_table(test_dir)\n",
        "  if row_id == 504:\n",
        "    train_dir = cwd + metadata['competition_name'] + '/data/train.json' \n",
        "    test_dir = cwd + metadata['competition_name'] + '/data/test.json'\n",
        "    train_df = pd.read_json(train_dir)\n",
        "    test_df = pd.read_json(test_dir)\n",
        "    \n",
        "  X_train, X_test, y_train, y_test, y_test_age, y_train_age, test, test_data = preprocessing(train_df, test_df)\n",
        "  X_train_featured, X_test_featured = feature_extraction(X_train, X_test, y_train, y_test)\n",
        "  estimation(X_train, X_test, y_train, y_test, test, test_data, X_train_featured, X_test_featured, y_test_age, y_train_age)\n",
        "  print(\"************************************************************\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "************************************************************\n",
            "digit-recognizer\n",
            "['pixel0', 'pixel783']\n",
            "['label']\n",
            "[]\n",
            "Label\n",
            "accuracy\n",
            "\n",
            "RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            " - 290s - loss: 0.4023 - acc: 0.8720 - val_loss: 0.0553 - val_acc: 0.9824\n",
            "************************************************************\n",
            "************************************************************\n",
            "labelme-lets-eat\n",
            "['Pixel0', 'Pixel1', 'Pixel98']\n",
            "[]\n",
            "[]\n",
            "Label\n",
            "accuracy\n",
            "\n",
            "AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "training_accuracy / validation_accuracy => 0.38 / 0.30 for step 0\n",
            "training_accuracy / validation_accuracy => 0.20 / 0.30 for step 1\n",
            "training_accuracy / validation_accuracy => 0.28 / 0.30 for step 2\n",
            "training_accuracy / validation_accuracy => 0.40 / 0.45 for step 3\n",
            "training_accuracy / validation_accuracy => 0.43 / 0.30 for step 4\n",
            "training_accuracy / validation_accuracy => 0.55 / 0.32 for step 5\n",
            "training_accuracy / validation_accuracy => 0.60 / 0.32 for step 6\n",
            "training_accuracy / validation_accuracy => 0.38 / 0.32 for step 7\n",
            "training_accuracy / validation_accuracy => 0.32 / 0.32 for step 8\n",
            "training_accuracy / validation_accuracy => 0.40 / 0.40 for step 9\n",
            "training_accuracy / validation_accuracy => 0.60 / 0.43 for step 10\n",
            "training_accuracy / validation_accuracy => 0.62 / 0.55 for step 20\n",
            "training_accuracy / validation_accuracy => 0.68 / 0.52 for step 30\n",
            "training_accuracy / validation_accuracy => 0.47 / 0.45 for step 40\n",
            "training_accuracy / validation_accuracy => 0.55 / 0.57 for step 50\n",
            "training_accuracy / validation_accuracy => 0.70 / 0.55 for step 60\n",
            "training_accuracy / validation_accuracy => 0.62 / 0.60 for step 70\n",
            "training_accuracy / validation_accuracy => 0.57 / 0.47 for step 80\n",
            "training_accuracy / validation_accuracy => 0.60 / 0.65 for step 90\n",
            "training_accuracy / validation_accuracy => 0.57 / 0.55 for step 100\n",
            "training_accuracy / validation_accuracy => 0.82 / 0.62 for step 200\n",
            "training_accuracy / validation_accuracy => 0.82 / 0.70 for step 300\n",
            "training_accuracy / validation_accuracy => 0.90 / 0.60 for step 400\n",
            "training_accuracy / validation_accuracy => 0.98 / 0.73 for step 500\n",
            "training_accuracy / validation_accuracy => 1.00 / 0.77 for step 600\n",
            "training_accuracy / validation_accuracy => 1.00 / 0.68 for step 700\n",
            "training_accuracy / validation_accuracy => 1.00 / 0.75 for step 800\n",
            "training_accuracy / validation_accuracy => 1.00 / 0.65 for step 900\n",
            "training_accuracy / validation_accuracy => 0.98 / 0.75 for step 999\n",
            "validation_accuracy => 0.6625\n",
            "************************************************************\n",
            "************************************************************\n",
            "nomad2018-predict-transparent-conductors\n",
            "['id', 'spacegroup', 'number_of_total_atoms', 'percent_atom_al', 'percent_atom_ga', 'percent_atom_in', 'lattice_vector_1_ang', 'lattice_vector_2_ang', 'lattice_vector_3_ang', 'lattice_angle_alpha_degree', 'lattice_angle_beta_degree', 'lattice_angle_gamma_degree', 'formation_energy_ev_natom', 'bandgap_energy_ev']\n",
            "[]\n",
            "[]\n",
            "bandgap_energy_ev, formation_energy_ev_natom\n",
            "rmsle\n",
            "\n",
            "regressor.compile(optimizer = 'adam', loss = 'mse', metrics =['accuracy'])\n",
            "Train on 2160 samples, validate on 240 samples\n",
            "Epoch 1/50\n",
            "2160/2160 [==============================] - 10s 4ms/step - loss: 0.1429 - acc: 0.9852 - val_loss: 0.0358 - val_acc: 0.9958\n",
            "Epoch 2/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0718 - acc: 0.9875 - val_loss: 0.0414 - val_acc: 0.9958\n",
            "Epoch 3/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0725 - acc: 0.9870 - val_loss: 0.0370 - val_acc: 0.9958\n",
            "Epoch 4/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0665 - acc: 0.9870 - val_loss: 0.0449 - val_acc: 0.9958\n",
            "Epoch 5/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0691 - acc: 0.9870 - val_loss: 0.0630 - val_acc: 0.9958\n",
            "Epoch 6/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0602 - acc: 0.9870 - val_loss: 0.0283 - val_acc: 0.9958\n",
            "Epoch 7/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0539 - acc: 0.9870 - val_loss: 0.0381 - val_acc: 0.9958\n",
            "Epoch 8/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0554 - acc: 0.9866 - val_loss: 0.0323 - val_acc: 0.9958\n",
            "Epoch 9/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0552 - acc: 0.9866 - val_loss: 0.0329 - val_acc: 0.9958\n",
            "Epoch 10/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0485 - acc: 0.9861 - val_loss: 0.0275 - val_acc: 0.9958\n",
            "Epoch 11/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0505 - acc: 0.9856 - val_loss: 0.0280 - val_acc: 0.9958\n",
            "Epoch 12/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0504 - acc: 0.9875 - val_loss: 0.0321 - val_acc: 0.9958\n",
            "Epoch 13/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0461 - acc: 0.9866 - val_loss: 0.0258 - val_acc: 0.9958\n",
            "Epoch 14/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0469 - acc: 0.9875 - val_loss: 0.0313 - val_acc: 0.9917\n",
            "Epoch 15/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0465 - acc: 0.9884 - val_loss: 0.0248 - val_acc: 0.9958\n",
            "Epoch 16/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0443 - acc: 0.9889 - val_loss: 0.0288 - val_acc: 0.9917\n",
            "Epoch 17/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0473 - acc: 0.9847 - val_loss: 0.0419 - val_acc: 0.9958\n",
            "Epoch 18/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0398 - acc: 0.9880 - val_loss: 0.0246 - val_acc: 0.9917\n",
            "Epoch 19/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0441 - acc: 0.9856 - val_loss: 0.0320 - val_acc: 0.9833\n",
            "Epoch 20/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0414 - acc: 0.9856 - val_loss: 0.0312 - val_acc: 0.9917\n",
            "Epoch 21/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0392 - acc: 0.9880 - val_loss: 0.0296 - val_acc: 0.9917\n",
            "Epoch 22/50\n",
            "2160/2160 [==============================] - 10s 5ms/step - loss: 0.0446 - acc: 0.9856 - val_loss: 0.0348 - val_acc: 0.9917\n",
            "Epoch 23/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0386 - acc: 0.9843 - val_loss: 0.0229 - val_acc: 0.9958\n",
            "Epoch 24/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0390 - acc: 0.9866 - val_loss: 0.0342 - val_acc: 0.9917\n",
            "Epoch 25/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0390 - acc: 0.9875 - val_loss: 0.0264 - val_acc: 0.9958\n",
            "Epoch 26/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0411 - acc: 0.9856 - val_loss: 0.0249 - val_acc: 0.9958\n",
            "Epoch 27/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0402 - acc: 0.9866 - val_loss: 0.0340 - val_acc: 0.9958\n",
            "Epoch 28/50\n",
            "2160/2160 [==============================] - 10s 4ms/step - loss: 0.0409 - acc: 0.9852 - val_loss: 0.0291 - val_acc: 0.9917\n",
            "Epoch 29/50\n",
            "2160/2160 [==============================] - 10s 5ms/step - loss: 0.0366 - acc: 0.9843 - val_loss: 0.0260 - val_acc: 0.9958\n",
            "Epoch 30/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0373 - acc: 0.9866 - val_loss: 0.0238 - val_acc: 0.9917\n",
            "Epoch 31/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0386 - acc: 0.9852 - val_loss: 0.0273 - val_acc: 0.9917\n",
            "Epoch 32/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0368 - acc: 0.9880 - val_loss: 0.0544 - val_acc: 0.9958\n",
            "Epoch 33/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0355 - acc: 0.9856 - val_loss: 0.0275 - val_acc: 0.9917\n",
            "Epoch 34/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0365 - acc: 0.9870 - val_loss: 0.0272 - val_acc: 0.9917\n",
            "Epoch 35/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0359 - acc: 0.9866 - val_loss: 0.0504 - val_acc: 0.9917\n",
            "Epoch 36/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0372 - acc: 0.9866 - val_loss: 0.0275 - val_acc: 0.9958\n",
            "Epoch 37/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0365 - acc: 0.9847 - val_loss: 0.0381 - val_acc: 0.9958\n",
            "Epoch 38/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0348 - acc: 0.9861 - val_loss: 0.0286 - val_acc: 0.9958\n",
            "Epoch 39/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0359 - acc: 0.9843 - val_loss: 0.0276 - val_acc: 0.9917\n",
            "Epoch 40/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0365 - acc: 0.9861 - val_loss: 0.0277 - val_acc: 0.9917\n",
            "Epoch 41/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0362 - acc: 0.9852 - val_loss: 0.0241 - val_acc: 0.9958\n",
            "Epoch 42/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0352 - acc: 0.9875 - val_loss: 0.0258 - val_acc: 0.9958\n",
            "Epoch 43/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0347 - acc: 0.9852 - val_loss: 0.0473 - val_acc: 0.9958\n",
            "Epoch 44/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0373 - acc: 0.9861 - val_loss: 0.0316 - val_acc: 0.9958\n",
            "Epoch 45/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0342 - acc: 0.9889 - val_loss: 0.0280 - val_acc: 0.9958\n",
            "Epoch 46/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0356 - acc: 0.9829 - val_loss: 0.0259 - val_acc: 0.9958\n",
            "Epoch 47/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0331 - acc: 0.9838 - val_loss: 0.0257 - val_acc: 0.9958\n",
            "Epoch 48/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0365 - acc: 0.9838 - val_loss: 0.0281 - val_acc: 0.9917\n",
            "Epoch 49/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0328 - acc: 0.9866 - val_loss: 0.0556 - val_acc: 0.9875\n",
            "Epoch 50/50\n",
            "2160/2160 [==============================] - 9s 4ms/step - loss: 0.0344 - acc: 0.9856 - val_loss: 0.0295 - val_acc: 0.9958\n",
            "0.06761258297256666\n",
            "************************************************************\n",
            "************************************************************\n",
            "spooky-author-identification\n",
            "[]\n",
            "[]\n",
            "['id', 'text', 'author']\n",
            "EAP, HPL, MWS\n",
            "logloss\n",
            "\n",
            "logisticRegressor = linear_model.LogisticRegression(C=0.01, solver='sag')\n",
            "----------------------------------------------------\n",
            "fitting \"CountVectorizer()\" for bag of char 5-grams\n",
            "----------------------------------------------------\n",
            "fitting \"CountVectorizer()\" for bag of word 2-grams\n",
            "fitting \"LogisticRegression()\" classifier\n",
            "Train: 99.7% Accuracy, log loss = 0.1271\n",
            "Valid: 87.4% Accuracy, log loss = 0.3620\n",
            "************************************************************\n",
            "************************************************************\n",
            "stumbleupon\n",
            "['urlid', 'alchemy_category_score', 'avglinksize', 'commonLinkRatio_1', 'commonLinkRatio_2', 'commonLinkRatio_3', 'commonLinkRatio_4', 'compression_ratio', 'embed_ratio', 'frameTagRatio', 'html_ratio', 'image_ratio', 'linkwordscore', 'non_markup_alphanum_characters', 'numberOfLinks', 'numwords_in_url', 'parametrizedLinkRatio', 'spelling_errors_ratio']\n",
            "[]\n",
            "['url', 'boilerplate', 'alchemy_category', 'frameBased', 'hasDomainLink', 'is_news', 'lengthyLinkDomain', 'news_front_page']\n",
            "label\n",
            "auc\n",
            "\n",
            "LogisticRegression(penalty='l2', dual=True, tol=0.0001, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None, random_state=None)\n",
            "20 Fold CV Score:  0.8771223762342266\n",
            "************************************************************\n",
            "************************************************************\n",
            "two-sigma-connect-rental-listing-inquiries\n",
            "['bedrooms', 'latitude', 'listing_id', 'longitude', 'price']\n",
            "['bathrooms', 'interest_level']\n",
            "['building_id', 'created', 'description', 'display_address', 'manager_id']\n",
            "High, medium, low\n",
            "logloss\n",
            "\n",
            "xgb.train(param, xgtrain, num_rounds, watchlist) param['objective'] = 'multi:softprob' param['eta'] = 0.02 param['max_depth'] = 6 param['silent'] = 1 param['num_class'] = 3 param['eval_metric'] = \"mlogloss\" param['min_child_weight'] = 3 param['subsample'] = 0.7 param['colsample_bytree'] = 0.7 param['seed'] = 321 param['nthread'] = 8 param['verbose'] = 1 param['print_every_n'] = 1\n",
            "Starting transformations\n",
            "Normalizing high cordiality data...\n",
            "[0]\ttrain-mlogloss:1.08518\n",
            "[1]\ttrain-mlogloss:1.07238\n",
            "[2]\ttrain-mlogloss:1.05962\n",
            "[3]\ttrain-mlogloss:1.04718\n",
            "[4]\ttrain-mlogloss:1.03512\n",
            "[5]\ttrain-mlogloss:1.0235\n",
            "[6]\ttrain-mlogloss:1.01246\n",
            "[7]\ttrain-mlogloss:1.00164\n",
            "[8]\ttrain-mlogloss:0.990989\n",
            "[9]\ttrain-mlogloss:0.980609\n",
            "[10]\ttrain-mlogloss:0.970932\n",
            "[11]\ttrain-mlogloss:0.961259\n",
            "[12]\ttrain-mlogloss:0.951997\n",
            "[13]\ttrain-mlogloss:0.942844\n",
            "[14]\ttrain-mlogloss:0.933925\n",
            "[15]\ttrain-mlogloss:0.925398\n",
            "[16]\ttrain-mlogloss:0.91702\n",
            "[17]\ttrain-mlogloss:0.908816\n",
            "[18]\ttrain-mlogloss:0.900845\n",
            "[19]\ttrain-mlogloss:0.893252\n",
            "[20]\ttrain-mlogloss:0.885693\n",
            "[21]\ttrain-mlogloss:0.878337\n",
            "[22]\ttrain-mlogloss:0.871302\n",
            "[23]\ttrain-mlogloss:0.864352\n",
            "[24]\ttrain-mlogloss:0.857791\n",
            "[25]\ttrain-mlogloss:0.851162\n",
            "[26]\ttrain-mlogloss:0.844634\n",
            "[27]\ttrain-mlogloss:0.838254\n",
            "[28]\ttrain-mlogloss:0.832188\n",
            "[29]\ttrain-mlogloss:0.826277\n",
            "[30]\ttrain-mlogloss:0.820406\n",
            "[31]\ttrain-mlogloss:0.814696\n",
            "[32]\ttrain-mlogloss:0.809098\n",
            "[33]\ttrain-mlogloss:0.803728\n",
            "[34]\ttrain-mlogloss:0.798509\n",
            "[35]\ttrain-mlogloss:0.793317\n",
            "[36]\ttrain-mlogloss:0.788195\n",
            "[37]\ttrain-mlogloss:0.783204\n",
            "[38]\ttrain-mlogloss:0.77848\n",
            "[39]\ttrain-mlogloss:0.773722\n",
            "[40]\ttrain-mlogloss:0.76912\n",
            "[41]\ttrain-mlogloss:0.764669\n",
            "[42]\ttrain-mlogloss:0.760332\n",
            "[43]\ttrain-mlogloss:0.756092\n",
            "[44]\ttrain-mlogloss:0.751857\n",
            "[45]\ttrain-mlogloss:0.747753\n",
            "[46]\ttrain-mlogloss:0.743842\n",
            "[47]\ttrain-mlogloss:0.739972\n",
            "[48]\ttrain-mlogloss:0.73621\n",
            "[49]\ttrain-mlogloss:0.732506\n",
            "[50]\ttrain-mlogloss:0.728785\n",
            "[51]\ttrain-mlogloss:0.725287\n",
            "[52]\ttrain-mlogloss:0.721774\n",
            "[53]\ttrain-mlogloss:0.718383\n",
            "[54]\ttrain-mlogloss:0.715034\n",
            "[55]\ttrain-mlogloss:0.711681\n",
            "[56]\ttrain-mlogloss:0.708555\n",
            "[57]\ttrain-mlogloss:0.705436\n",
            "[58]\ttrain-mlogloss:0.702365\n",
            "[59]\ttrain-mlogloss:0.699383\n",
            "[60]\ttrain-mlogloss:0.696418\n",
            "[61]\ttrain-mlogloss:0.693538\n",
            "[62]\ttrain-mlogloss:0.690853\n",
            "[63]\ttrain-mlogloss:0.688147\n",
            "[64]\ttrain-mlogloss:0.685374\n",
            "[65]\ttrain-mlogloss:0.682743\n",
            "[66]\ttrain-mlogloss:0.68017\n",
            "[67]\ttrain-mlogloss:0.677662\n",
            "[68]\ttrain-mlogloss:0.67518\n",
            "[69]\ttrain-mlogloss:0.672795\n",
            "[70]\ttrain-mlogloss:0.670391\n",
            "[71]\ttrain-mlogloss:0.668054\n",
            "[72]\ttrain-mlogloss:0.665741\n",
            "[73]\ttrain-mlogloss:0.663489\n",
            "[74]\ttrain-mlogloss:0.661293\n",
            "[75]\ttrain-mlogloss:0.659139\n",
            "[76]\ttrain-mlogloss:0.657023\n",
            "[77]\ttrain-mlogloss:0.654982\n",
            "[78]\ttrain-mlogloss:0.653002\n",
            "[79]\ttrain-mlogloss:0.650989\n",
            "[80]\ttrain-mlogloss:0.648981\n",
            "[81]\ttrain-mlogloss:0.647121\n",
            "[82]\ttrain-mlogloss:0.645315\n",
            "[83]\ttrain-mlogloss:0.643449\n",
            "[84]\ttrain-mlogloss:0.641648\n",
            "[85]\ttrain-mlogloss:0.639857\n",
            "[86]\ttrain-mlogloss:0.63815\n",
            "[87]\ttrain-mlogloss:0.636489\n",
            "[88]\ttrain-mlogloss:0.634774\n",
            "[89]\ttrain-mlogloss:0.633112\n",
            "[90]\ttrain-mlogloss:0.631525\n",
            "[91]\ttrain-mlogloss:0.629955\n",
            "[92]\ttrain-mlogloss:0.628426\n",
            "[93]\ttrain-mlogloss:0.626842\n",
            "[94]\ttrain-mlogloss:0.625318\n",
            "[95]\ttrain-mlogloss:0.623846\n",
            "[96]\ttrain-mlogloss:0.622459\n",
            "[97]\ttrain-mlogloss:0.621024\n",
            "[98]\ttrain-mlogloss:0.619568\n",
            "[99]\ttrain-mlogloss:0.618181\n",
            "[100]\ttrain-mlogloss:0.616833\n",
            "[101]\ttrain-mlogloss:0.615515\n",
            "[102]\ttrain-mlogloss:0.614239\n",
            "[103]\ttrain-mlogloss:0.612977\n",
            "[104]\ttrain-mlogloss:0.611729\n",
            "[105]\ttrain-mlogloss:0.610486\n",
            "[106]\ttrain-mlogloss:0.609303\n",
            "[107]\ttrain-mlogloss:0.608098\n",
            "[108]\ttrain-mlogloss:0.606962\n",
            "[109]\ttrain-mlogloss:0.605799\n",
            "[110]\ttrain-mlogloss:0.604785\n",
            "[111]\ttrain-mlogloss:0.603643\n",
            "[112]\ttrain-mlogloss:0.602496\n",
            "[113]\ttrain-mlogloss:0.601355\n",
            "[114]\ttrain-mlogloss:0.600324\n",
            "[115]\ttrain-mlogloss:0.599292\n",
            "[116]\ttrain-mlogloss:0.59832\n",
            "[117]\ttrain-mlogloss:0.597274\n",
            "[118]\ttrain-mlogloss:0.59625\n",
            "[119]\ttrain-mlogloss:0.595281\n",
            "[120]\ttrain-mlogloss:0.594342\n",
            "[121]\ttrain-mlogloss:0.593408\n",
            "[122]\ttrain-mlogloss:0.592456\n",
            "[123]\ttrain-mlogloss:0.591517\n",
            "[124]\ttrain-mlogloss:0.590659\n",
            "[125]\ttrain-mlogloss:0.58976\n",
            "[126]\ttrain-mlogloss:0.588878\n",
            "[127]\ttrain-mlogloss:0.588004\n",
            "[128]\ttrain-mlogloss:0.587137\n",
            "[129]\ttrain-mlogloss:0.586329\n",
            "[130]\ttrain-mlogloss:0.585442\n",
            "[131]\ttrain-mlogloss:0.584621\n",
            "[132]\ttrain-mlogloss:0.583817\n",
            "[133]\ttrain-mlogloss:0.583043\n",
            "[134]\ttrain-mlogloss:0.582223\n",
            "[135]\ttrain-mlogloss:0.581472\n",
            "[136]\ttrain-mlogloss:0.580742\n",
            "[137]\ttrain-mlogloss:0.579999\n",
            "[138]\ttrain-mlogloss:0.579251\n",
            "[139]\ttrain-mlogloss:0.57855\n",
            "[140]\ttrain-mlogloss:0.577769\n",
            "[141]\ttrain-mlogloss:0.577071\n",
            "[142]\ttrain-mlogloss:0.57641\n",
            "[143]\ttrain-mlogloss:0.575717\n",
            "[144]\ttrain-mlogloss:0.575067\n",
            "[145]\ttrain-mlogloss:0.574376\n",
            "[146]\ttrain-mlogloss:0.573757\n",
            "[147]\ttrain-mlogloss:0.573124\n",
            "[148]\ttrain-mlogloss:0.572479\n",
            "[149]\ttrain-mlogloss:0.57177\n",
            "[150]\ttrain-mlogloss:0.571072\n",
            "[151]\ttrain-mlogloss:0.570418\n",
            "[152]\ttrain-mlogloss:0.569747\n",
            "[153]\ttrain-mlogloss:0.569101\n",
            "[154]\ttrain-mlogloss:0.568529\n",
            "[155]\ttrain-mlogloss:0.567869\n",
            "[156]\ttrain-mlogloss:0.567296\n",
            "[157]\ttrain-mlogloss:0.566699\n",
            "[158]\ttrain-mlogloss:0.566107\n",
            "[159]\ttrain-mlogloss:0.565554\n",
            "[160]\ttrain-mlogloss:0.565019\n",
            "[161]\ttrain-mlogloss:0.56443\n",
            "[162]\ttrain-mlogloss:0.563841\n",
            "[163]\ttrain-mlogloss:0.563282\n",
            "[164]\ttrain-mlogloss:0.562693\n",
            "[165]\ttrain-mlogloss:0.562114\n",
            "[166]\ttrain-mlogloss:0.561576\n",
            "[167]\ttrain-mlogloss:0.560957\n",
            "[168]\ttrain-mlogloss:0.560452\n",
            "[169]\ttrain-mlogloss:0.559959\n",
            "[170]\ttrain-mlogloss:0.559526\n",
            "[171]\ttrain-mlogloss:0.559016\n",
            "[172]\ttrain-mlogloss:0.558505\n",
            "[173]\ttrain-mlogloss:0.558048\n",
            "[174]\ttrain-mlogloss:0.557573\n",
            "[175]\ttrain-mlogloss:0.557044\n",
            "[176]\ttrain-mlogloss:0.556568\n",
            "[177]\ttrain-mlogloss:0.556142\n",
            "[178]\ttrain-mlogloss:0.555718\n",
            "[179]\ttrain-mlogloss:0.555253\n",
            "[180]\ttrain-mlogloss:0.554815\n",
            "[181]\ttrain-mlogloss:0.554391\n",
            "[182]\ttrain-mlogloss:0.553946\n",
            "[183]\ttrain-mlogloss:0.553513\n",
            "[184]\ttrain-mlogloss:0.553076\n",
            "[185]\ttrain-mlogloss:0.552684\n",
            "[186]\ttrain-mlogloss:0.552261\n",
            "[187]\ttrain-mlogloss:0.551826\n",
            "[188]\ttrain-mlogloss:0.551351\n",
            "[189]\ttrain-mlogloss:0.550976\n",
            "[190]\ttrain-mlogloss:0.550561\n",
            "[191]\ttrain-mlogloss:0.550144\n",
            "[192]\ttrain-mlogloss:0.549785\n",
            "[193]\ttrain-mlogloss:0.549314\n",
            "[194]\ttrain-mlogloss:0.548952\n",
            "[195]\ttrain-mlogloss:0.548569\n",
            "[196]\ttrain-mlogloss:0.54814\n",
            "[197]\ttrain-mlogloss:0.547674\n",
            "[198]\ttrain-mlogloss:0.547278\n",
            "[199]\ttrain-mlogloss:0.54691\n",
            "[200]\ttrain-mlogloss:0.546408\n",
            "[201]\ttrain-mlogloss:0.546054\n",
            "[202]\ttrain-mlogloss:0.545658\n",
            "[203]\ttrain-mlogloss:0.545258\n",
            "[204]\ttrain-mlogloss:0.544942\n",
            "[205]\ttrain-mlogloss:0.544527\n",
            "[206]\ttrain-mlogloss:0.544156\n",
            "[207]\ttrain-mlogloss:0.543822\n",
            "[208]\ttrain-mlogloss:0.543467\n",
            "[209]\ttrain-mlogloss:0.543088\n",
            "[210]\ttrain-mlogloss:0.542617\n",
            "[211]\ttrain-mlogloss:0.542271\n",
            "[212]\ttrain-mlogloss:0.541868\n",
            "[213]\ttrain-mlogloss:0.541501\n",
            "[214]\ttrain-mlogloss:0.541108\n",
            "[215]\ttrain-mlogloss:0.540798\n",
            "[216]\ttrain-mlogloss:0.540363\n",
            "[217]\ttrain-mlogloss:0.54003\n",
            "[218]\ttrain-mlogloss:0.539642\n",
            "[219]\ttrain-mlogloss:0.539313\n",
            "[220]\ttrain-mlogloss:0.538965\n",
            "[221]\ttrain-mlogloss:0.53864\n",
            "[222]\ttrain-mlogloss:0.538307\n",
            "[223]\ttrain-mlogloss:0.537969\n",
            "[224]\ttrain-mlogloss:0.537659\n",
            "[225]\ttrain-mlogloss:0.537366\n",
            "[226]\ttrain-mlogloss:0.537058\n",
            "[227]\ttrain-mlogloss:0.53674\n",
            "[228]\ttrain-mlogloss:0.53642\n",
            "[229]\ttrain-mlogloss:0.53609\n",
            "[230]\ttrain-mlogloss:0.535728\n",
            "[231]\ttrain-mlogloss:0.5354\n",
            "[232]\ttrain-mlogloss:0.535102\n",
            "[233]\ttrain-mlogloss:0.534838\n",
            "[234]\ttrain-mlogloss:0.534451\n",
            "[235]\ttrain-mlogloss:0.534155\n",
            "[236]\ttrain-mlogloss:0.533889\n",
            "[237]\ttrain-mlogloss:0.533555\n",
            "[238]\ttrain-mlogloss:0.533199\n",
            "[239]\ttrain-mlogloss:0.532917\n",
            "[240]\ttrain-mlogloss:0.532623\n",
            "[241]\ttrain-mlogloss:0.532383\n",
            "[242]\ttrain-mlogloss:0.532073\n",
            "[243]\ttrain-mlogloss:0.531779\n",
            "[244]\ttrain-mlogloss:0.531471\n",
            "[245]\ttrain-mlogloss:0.531168\n",
            "[246]\ttrain-mlogloss:0.530816\n",
            "[247]\ttrain-mlogloss:0.530523\n",
            "[248]\ttrain-mlogloss:0.530262\n",
            "[249]\ttrain-mlogloss:0.529961\n",
            "[250]\ttrain-mlogloss:0.529575\n",
            "[251]\ttrain-mlogloss:0.529306\n",
            "[252]\ttrain-mlogloss:0.529081\n",
            "[253]\ttrain-mlogloss:0.5288\n",
            "[254]\ttrain-mlogloss:0.528517\n",
            "[255]\ttrain-mlogloss:0.528281\n",
            "[256]\ttrain-mlogloss:0.528052\n",
            "[257]\ttrain-mlogloss:0.527765\n",
            "[258]\ttrain-mlogloss:0.527528\n",
            "[259]\ttrain-mlogloss:0.527268\n",
            "[260]\ttrain-mlogloss:0.526985\n",
            "[261]\ttrain-mlogloss:0.526682\n",
            "[262]\ttrain-mlogloss:0.526409\n",
            "[263]\ttrain-mlogloss:0.526119\n",
            "[264]\ttrain-mlogloss:0.525826\n",
            "[265]\ttrain-mlogloss:0.525592\n",
            "[266]\ttrain-mlogloss:0.525345\n",
            "[267]\ttrain-mlogloss:0.52508\n",
            "[268]\ttrain-mlogloss:0.524824\n",
            "[269]\ttrain-mlogloss:0.524592\n",
            "[270]\ttrain-mlogloss:0.524279\n",
            "[271]\ttrain-mlogloss:0.524047\n",
            "[272]\ttrain-mlogloss:0.523789\n",
            "[273]\ttrain-mlogloss:0.523511\n",
            "[274]\ttrain-mlogloss:0.523274\n",
            "[275]\ttrain-mlogloss:0.523025\n",
            "[276]\ttrain-mlogloss:0.522782\n",
            "[277]\ttrain-mlogloss:0.522503\n",
            "[278]\ttrain-mlogloss:0.522285\n",
            "[279]\ttrain-mlogloss:0.522025\n",
            "[280]\ttrain-mlogloss:0.52176\n",
            "[281]\ttrain-mlogloss:0.521501\n",
            "[282]\ttrain-mlogloss:0.521277\n",
            "[283]\ttrain-mlogloss:0.521062\n",
            "[284]\ttrain-mlogloss:0.520778\n",
            "[285]\ttrain-mlogloss:0.520526\n",
            "[286]\ttrain-mlogloss:0.520301\n",
            "[287]\ttrain-mlogloss:0.520101\n",
            "[288]\ttrain-mlogloss:0.519861\n",
            "[289]\ttrain-mlogloss:0.519657\n",
            "[290]\ttrain-mlogloss:0.519387\n",
            "[291]\ttrain-mlogloss:0.519147\n",
            "[292]\ttrain-mlogloss:0.518907\n",
            "[293]\ttrain-mlogloss:0.518661\n",
            "[294]\ttrain-mlogloss:0.518432\n",
            "[295]\ttrain-mlogloss:0.51817\n",
            "[296]\ttrain-mlogloss:0.517975\n",
            "[297]\ttrain-mlogloss:0.517761\n",
            "[298]\ttrain-mlogloss:0.5175\n",
            "[299]\ttrain-mlogloss:0.517223\n",
            "Fitted\n",
            "************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "978y60OOzxm4",
        "colab_type": "code",
        "outputId": "b09b99f9-f749-4c5c-e525-2c5a14f0e7ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1508.5137917995453\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}