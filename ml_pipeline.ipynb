{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“ml_pipeline.ipynb”的副本",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "TxFQTU7wwH6z",
        "28ePEm2TzDLX",
        "1SjowYZNf1cm",
        "M4WBvMQ6zOLg",
        "OyJWGIUyXbeS",
        "Zd2flwegzW1A",
        "4K3BYEqzzcPo",
        "5pVGCiD6QFb3",
        "tedJ6NDxQadI",
        "0t-lM6gfzhCx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AMTnmq1v-7S",
        "colab_type": "text"
      },
      "source": [
        "# Google Colab Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc60ZxQ6GdDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import gc \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn.svm as svm\n",
        "import sklearn.tree as tree\n",
        "import sklearn.ensemble as ensemble\n",
        "import sklearn.neighbors as neighbors\n",
        "import sklearn.naive_bayes as naive_bayes\n",
        "import sklearn.linear_model as linear_model\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing as preproc\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error, mean_absolute_error, roc_curve, auc, confusion_matrix\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_vMpcWR6fUq",
        "colab_type": "code",
        "outputId": "8a0a77c1-9dab-49c7-b347-3c271134dcd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "# Accessing Google sheets\n",
        "!pip install --upgrade -q gspread\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "worksheet = gc.open('Metadata1').worksheet('Sheet1')\n",
        "\n",
        "# get_all_values gives a list of rows\n",
        "rows_ = worksheet.get_all_values()\n",
        "print(rows_)\n",
        "\n",
        "#rows = pd.read_excel('Metadata.xlsx')\n",
        "# Convert to a DataFrame and render.\n",
        "#import pandas as pd\n",
        "rows = pd.DataFrame.from_records(rows_)\n",
        "print(rows)\n",
        "\n",
        "new_header = rows.iloc[0] #grab the first row for the header\n",
        "rows = rows[1:] #take the data less the header row\n",
        "rows.columns = new_header #set the header row as the df header"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Id', '', 'name', 'URL', 'endDate', 'dataSize', 'table', 'image', 'audio', 'video', 'data type', 'text/csv', 'text/json', 'text/tab-separated-values', 'image/bmp', 'image/jpeg', 'image/png', 'image/tiff', 'audio/x-wav', 'audio/x-aiff', 'video/mp4', 'data format', 'columns [index;name;type;...] for type use categorical, numerical, string, integer, dateTime etc', 'augmented dataset URL', 'taskType', 'taskSubType', 'outputType', 'targetIndex', 'targetName', 'rawData (non csv) ', 'rawDataIndex', 'problemDescription', 'preprocessing', 'preprocessing function call', 'featureExtractor', 'featureExtractor function call', 'featureSelector', 'featureSelector function call', 'sklearn', 'xgboost', 'keras', 'tensorflow', 'lightgbm', 'Libraries', 'estimators', 'estimator1', 'estimator1 function call', 'estimator2', 'estimator2 function call', 'estimator3', 'estimator3 function call', 'postprocessing', 'postprocessing function call', 'performanceMetric', 'crossValidationPerformance', 'codeURIRunningTimeSecondsTesting', 'codeURIRunningTimeSecondsTraining', 'WellWrittenCodeDocRating0-5', 'WellWrittenCodeRating0-5', 'codeURI', 'codeYear', 'codeMonth', 'codeAuthor', 'codeCountry', 'GenericUnivariateSelect', 'SelectPercentile', 'SelectKBest', 'SelectFpr', 'SelectFdr', 'SelectFromModel', 'SelectFwe', 'RFE', 'RFECV', 'VarianceThreshold', 'chi2', 'f_classif', 'f_regression', 'mutual_info_classif', 'mutual_info_regression', 'KNeighborsClassifier', 'KNeighborsRegressor', 'SGDClassifier', 'LinearRegression', 'LogisticRegression', 'Ridge', 'BayesianRidge', 'Lasso', 'SGDRegressor', 'DecisionTreeClassifier', 'DecisionTreeRegressor', 'LinearSVC', 'SVC', 'LinearSVR', 'RandomForestClassifier', 'GradientBoostingClassifier', 'RandomForestRegressor', 'GradientBoostingRegressor', 'MLPClassifier', 'XGBClassifier', 'XGBRegressor', 'CNN', 'ResNet'], ['3', 'yz4953, jy2823', 'digit-recognizer', 'https://www.kaggle.com/c/digit-recognizer', '1/7/20 0:00', '15MB', 'True', 'False', 'False', 'False', 'table', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'csv', '[0; label;categorical;1; pixel0;integer;4; pixel783;integer]', '', 'classification', 'multiclass', 'classLabel', '1', 'Label', 'False', '', 'To recognize a handwritten digit', 'drop features & entries, normalization/scaling, one hot encoding', '', '', '', '', 'none', 'False', 'False', 'True', 'False', 'False', 'keras', 'cnn', 'cnn', 'Sequential()', '', 'RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)', '', '', '', '', 'accuracy', '0.997', '', 'B', '', '', 'https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6', '2017', 'August', 'Yassine Ghouzam', 'France', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False'], ['335', 'yz4953,jy2823', 'spooky-author-identification', 'https://www.kaggle.com/c/spooky-author-identification', '12/15/17 23:59', '4.7MB', 'True', 'False', 'False', 'False', 'table', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'csv', '[0; id;string;1; text;string;2; author;string]', '', 'classification', 'multiclass', 'probabilities, multiLabel', '1 to 3', 'author', 'False', '', 'Share code and discuss insights to identify horror authors from their writings', 'sklearn.model_selection, text pre-processing', '', 'countvectorizer', \"CountVectorizer(min_df=8, max_features=250000, analyzer='char', ngram_range=(1,ngramLength), binary=False,lowercase=True)\", '', 'none', 'True', 'False', 'False', 'False', 'False', 'sklearn', 'logisticregression', 'decisiontreeclassifier/regressor', \"linear_model.LogisticRegression(C=0.01, solver='sag')\", 'clustering', '', '', '', 'csv conversion', '', 'logloss', '0.1271', '', '', '', '', 'https://www.kaggle.com/selfishgene/generating-sentences-one-letter-at-a-time', '2017', 'December', 'Selfish Gene', 'Israel', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False'], ['534', 'jy2823, yz4953', 'titanic', 'https://www.kaggle.com/c/titanic', '4/7/20 0:00', '59KB', 'True', 'False', 'False', 'False', 'table', 'True', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'csv', '[0; PassengerId;integer;1; Survived;string;2; Pclass;integer;3; Name;string;4; Sex;categorical;5; Age;integer;6; SibSp;integer;7; Parch;integer;8; Ticket;string;9; Fare;real;10; Cabin;string;11; Embarked;categorical]', '', 'classification', 'binary', 'classLabel', '1', 'Survived', 'False', '', 'Predicting if a passenger survived the sinking of Titanic or not', 'data cleaning, data wrangling', '', '', '', '', 'SelectKBest(f_classif, k=10)', 'True', 'False', 'False', 'False', 'False', 'Sklearn', 'randomforestregressor/classifier', 'randomforestregressor/classifier', 'ensemble.RandomForestClassifier(n_estimators=100)', '', '', '', '', '', '', 'accuracy', '0.8676', '', '', '', '', 'https://www.kaggle.com/startupsci/titanic-data-science-solutions', '2017', 'December', 'Manav Sehgal', 'India', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False'], ['555', 'jy2823, yz4953', 'dogs-vs-cats-redux-kernels-edition', 'https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition', '3/2/17 23:59', '814MB', 'False', 'True', 'False', 'False', 'image', 'False', 'False', 'False', 'False', 'True', 'False', 'False', 'False', 'False', 'False', 'jpg', '[]', '', 'classification', 'binary', 'probabilities', '1', 'Label', 'True', '0', 'To recognize pictures of dogs', 'image pre-processing', '', 'convert data to arrays', '', '', 'none', 'False', 'False', 'False', 'True', 'False', 'TensorFlow', 'cnn', 'cnn', 'Sequential()', '', '', '', '', '', '', 'accuracy', '0.79', '', '', '', '', 'https://www.kaggle.com/risingdeveloper/dogs-vs-cats-keras-implementation', '2018', 'December', 'Rising Odegua', 'Nigeria', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'False', 'True', 'False']]\n",
            "   0               1    ...    100     101\n",
            "0   Id                  ...    CNN  ResNet\n",
            "1    3  yz4953, jy2823  ...   True   False\n",
            "2  335   yz4953,jy2823  ...  False   False\n",
            "3  534  jy2823, yz4953  ...  False   False\n",
            "4  555  jy2823, yz4953  ...   True   False\n",
            "\n",
            "[5 rows x 102 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK3MAgppnZC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import xlrd\n",
        "# # worksheet = xlrd.open_workbook('Metadata.xlsx').worksheet('Sheet1')\n",
        "\n",
        "# workbook = xlrd.open_workbook('Metadata.xlsx')\n",
        "\n",
        "# worksheet = workbook.sheet_by_index(0)\n",
        "# # get_all_values gives a list of rows\n",
        "# rows_ = worksheet.get_all_values()\n",
        "\n",
        "# #rows = pd.read_excel('Metadata.xlsx')\n",
        "# # Convert to a DataFrame and render.\n",
        "# #import pandas as pd\n",
        "# rows = pd.DataFrame.from_records(rows_)\n",
        "# print(rows)\n",
        "\n",
        "# new_header = rows.iloc[0] #grab the first row for the header\n",
        "# rows = rows[1:] #take the data less the header row\n",
        "# rows.columns = new_header #set the header row as the df header"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shhl7NnQ6gaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def alpha_to_number(alpha_key):\n",
        "  return sum([(ord(alpha)-64)*(26**ind) for ind, alpha in enumerate(list(alpha_key)[::-1])]) - 1\n",
        "\n",
        "# Mapping from Metadata sheet column name to readable columns\n",
        "column_key = {'name': 'C', 'columns': 'W', 'estimator_func_call': 'AU', 'target_name': 'AC', 'output_type': 'AA', 'performance_metric': 'BB', 'feature_selector': 'AL', 'data_form': 'V','feature_extractor':'AJ'}\n",
        "column_key = dict(map(lambda kv: (kv[0], alpha_to_number(kv[1])), column_key.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxFQTU7wwH6z",
        "colab_type": "text"
      },
      "source": [
        "# Mount at Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOwYBjH5wMuK",
        "colab_type": "text"
      },
      "source": [
        "If cannot read from the file,  please rerun this statement until \"gdrive/My Drive\" appears on the left bar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJG0sKlxtaSW",
        "colab_type": "code",
        "outputId": "903878b4-e0b2-422e-f1bf-033b3e27fc81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCQ3yt7Ywg0R",
        "colab_type": "text"
      },
      "source": [
        "# Metadata Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWeydbYR6oZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parseMetaData(row_id):\n",
        "  \n",
        "  \n",
        "  metadata['competition_name'] = rows.loc[row_id][column_key['name']]\n",
        "  metadata['estimator'] = rows.loc[row_id][column_key['estimator_func_call']]\n",
        "  metadata['target_column'] = rows.loc[row_id][column_key['target_name']]\n",
        "  metadata['output_type'] = rows.loc[row_id][column_key['output_type']].split(',')\n",
        "  metadata['metric'] = rows.loc[row_id][column_key['performance_metric']]\n",
        "  metadata['feature_selector'] = rows.loc[row_id][column_key['feature_selector']]\n",
        "  metadata['feature_extractor'] = rows.loc[row_id][column_key['feature_extractor']]\n",
        "  metadata['data_form'] = rows.loc[row_id][column_key['data_form']]\n",
        "  columns = rows.loc[row_id][column_key['columns']]\n",
        "\n",
        "  # Parse column information \n",
        "  numeric_columns = []\n",
        "  unwanted_columns = []\n",
        "  categorical_columns = []\n",
        "  columns_data = [x.strip() for x in columns[1:-1].split(';')]\n",
        "  #print(columns_data)\n",
        "  for ind, val in enumerate(columns_data):\n",
        "    if ind%3 == 2:\n",
        "      if (val == \"numeric\" or val == \"integer\" or val == \"real\"):\n",
        "        numeric_columns.append(columns_data[ind-1])\n",
        "      elif val == \"categorical\":\n",
        "        categorical_columns.append(columns_data[ind-1])\n",
        "      elif val == \"unwanted\" or val == \"string\" or val == 'dateTime':\n",
        "        unwanted_columns.append(columns_data[ind-1])\n",
        "    else:\n",
        "      pass \n",
        "  metadata['numeric_columns'] = numeric_columns\n",
        "  metadata['unwanted_columns'] = unwanted_columns\n",
        "  metadata['categorical_columns'] = categorical_columns\n",
        "  \n",
        "  # Remove target from features columns\n",
        "  if metadata['target_column'] in metadata['numeric_columns']:\n",
        "    metadata['numeric_columns'].remove(metadata['target_column'])\n",
        "  if metadata['target_column'] in metadata['categorical_columns']:\n",
        "    metadata['categorical_columns'].remove(metadata['target_column'])\n",
        "  if metadata['target_column'] in metadata['unwanted_columns']:\n",
        "    metadata['unwanted_columns'].remove(metadata['target_column'])\n",
        "  \n",
        "  print(metadata['competition_name'])\n",
        "  print(metadata['numeric_columns'])\n",
        "  print(metadata['categorical_columns'])\n",
        "  print(metadata['unwanted_columns'])\n",
        "  print(metadata['target_column'])\n",
        "  print(metadata['metric'])\n",
        "  print(metadata['feature_selector'])\n",
        "  print(metadata['feature_extractor'])\n",
        "  print(metadata['estimator'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28ePEm2TzDLX",
        "colab_type": "text"
      },
      "source": [
        "# Add relevent import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rRxxUSsjj_D",
        "colab_type": "code",
        "outputId": "d90ca6f8-0c97-4c94-b20e-ca492f2e6bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Installations\n",
        "import warnings\n",
        "import random\n",
        "from math import exp\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Imports\n",
        "# Preprocessing imports\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import itertools\n",
        "\n",
        "from keras.utils.np_utils import to_categorical \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, MaxPooling2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import scipy\n",
        "import re\n",
        "\n",
        "# Other initializations\n",
        "sns.set(style='white', context='notebook', palette='deep')\n",
        "epochs_completed = 0\n",
        "index_in_epoch = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SjowYZNf1cm",
        "colab_type": "text"
      },
      "source": [
        "# Data Auxiliary\n",
        "\n",
        "**Applied to Digit Recognizer**\n",
        "\n",
        "1.   Data Description:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Original training data set: 42.0k x 785 (label x 1, pixel x 784)\n",
        "*   Auxiliary training data set: 60.0k x 785 (label x 1, pixel x 784)\n",
        "\n",
        "> Reference: https://www.kaggle.com/krissa10/train-digit-recognition-mnist\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Concat training data set: 102.0k x 785 (label x 1, pixel x 784)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2.   Accuracy Performance:\n",
        "\n",
        "\n",
        "*   X_train after augmentaion :  0.8656 (training) 0.9783(validation) Running time: 1557084685.2798023s\n",
        "\n",
        "*   X_train combine with X_train_auxiliary after augmentation: 0.9271(training) 0.9893(validation) Running time: 1557088917.688372s\n",
        "\n",
        "We can observe a significant impovement in data training and validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iNTDDAhgKPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_auxi(train_df):\n",
        "  print(train_df.shape)\n",
        "  drive.mount('/content/gdrive')\n",
        "  cwd = 'gdrive/My Drive/mlProject/digit-recognizer'\n",
        "  aux_dir = cwd + '/auxiliary_data/train_auxiliary.csv'\n",
        "  aux_df = pd.read_csv(aux_dir)\n",
        "  train_df = pd.concat([train_df, aux_df], axis=0)\n",
        "  print(train_df.shape)\n",
        "  return train_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4WBvMQ6zOLg",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MMk3DPzDx-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(train_df):\n",
        "  if metadata['competition_name']=='dogs-vs-cats-redux-kernels-edition':\n",
        "    train_dogs = [train_df+'/dog/{}'.format(i) for i in os.listdir(train_df+'/dog') ]  #get dog images if 'dog' in i\n",
        "    train_cats = [train_df+'/cat/{}'.format(i) for i in os.listdir(train_df+'/cat') ]  #get cat images if 'cat' in i\n",
        "    train_imgs = train_dogs[2000:4000] + train_cats[2000:4000]  # slice the dataset and use 2000 in each class\n",
        "    random.shuffle(train_imgs)  # shuffle it randomly\n",
        "    #Clear list that are useless\n",
        "    del train_dogs\n",
        "    del train_cats\n",
        "    #gc.collect()   #collect garbage to save memory\n",
        "    nrows = 150\n",
        "    ncolumns = 150\n",
        "    channels = 3  #change to 1 if you want to use grayscale image\n",
        "    #get the train and label data\n",
        "    X, y = read_and_process_image(train_imgs,nrows,ncolumns)\n",
        "\n",
        "    import seaborn as sns\n",
        "    del train_imgs\n",
        "    #gc.collect()\n",
        "\n",
        "    #Convert list to numpy array\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2)\n",
        "\n",
        "    #clear memory\n",
        "    del X\n",
        "    del y\n",
        "    #gc.collect()\n",
        "  \n",
        "  else:  \n",
        "    #get X and y from input data\n",
        "    if metadata['competition_name']=='digit-recognizer':\n",
        "      train_df = create_auxi(train_df)\n",
        "      X = train_df.drop(metadata['target_column'], 1)\n",
        "      y = train_df[metadata['target_column']]\n",
        "      X = X / 255.0\n",
        "      # Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
        "      X = X.values.reshape(-1,28,28,1)\n",
        "      # Encode labels to one hot vectors\n",
        "      y = to_categorical(y, num_classes = (np.max(y)+1))\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=2)\n",
        "    \n",
        "    elif metadata['competition_name']=='spooky-author-identification':\n",
        "      #test_data = test_df.loc[:,'text'].reset_index(drop=True)\n",
        "      stratifiedCV = model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=1)\n",
        "      trainInds, validInds = next(stratifiedCV.split(train_df['text'], train_df['author']))\n",
        "      X_train = train_df.loc[trainInds,'text'].reset_index(drop=True)\n",
        "      X_test  = train_df.loc[validInds,'text'].reset_index(drop=True)\n",
        "      trainLabel = train_df.loc[trainInds,'author'].reset_index(drop=True)\n",
        "      validLabel = train_df.loc[validInds,'author'].reset_index(drop=True)\n",
        "      yLabelEncoder = preproc.LabelEncoder()\n",
        "      yLabelEncoder.fit(pd.concat((trainLabel,validLabel)))\n",
        "      y_train = yLabelEncoder.transform(trainLabel)\n",
        "      y_test = yLabelEncoder.transform(validLabel)\n",
        "  \n",
        "    else:\n",
        "      X = train_df.drop(metadata['target_column'], 1)\n",
        "      y = train_df[metadata['target_column']]\n",
        "      X = X.filter(metadata['numeric_columns'] + metadata['categorical_columns'])\n",
        "  \n",
        "      # treat missing values\n",
        "      pd.set_option('mode.chained_assignment', None) # used to subside the panda's chain assignment warning\n",
        "      imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "      for col in metadata['numeric_columns']:\n",
        "        X[[col]] = imp.fit_transform(X[[col]])\n",
        "    \n",
        "      # Categorial transform  \n",
        "      for col in metadata['categorical_columns']:\n",
        "        col_dummies = pd.get_dummies(X[col], dummy_na=True)\n",
        "        X = pd.concat([X, col_dummies], axis=1)\n",
        "      X.drop(metadata['categorical_columns'], axis=1, inplace=True)\n",
        "  \n",
        "      # Feature normalization\n",
        "      X[metadata['numeric_columns']] = preproc.scale(X[metadata['numeric_columns']])\n",
        "\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "  \n",
        "      \n",
        "  return X_train, X_test, y_train, y_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyJWGIUyXbeS",
        "colab_type": "text"
      },
      "source": [
        "# Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbOE5_lYXiLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A function to read and process the images to an acceptable format for our model\n",
        "def read_and_process_image(list_of_images, nrows,ncolumns,):\n",
        "    \"\"\"\n",
        "    Returns two arrays: \n",
        "        X is an array of resized images\n",
        "        y is an array of labels\n",
        "    \"\"\"\n",
        "    X = [] # images\n",
        "    y = [] # labels\n",
        "    \n",
        "    for image in list_of_images:\n",
        "        X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (nrows,ncolumns), interpolation=cv2.INTER_CUBIC))  #Read the image\n",
        "        #get the labels\n",
        "        if 'dog' in image:\n",
        "            y.append(1)\n",
        "        elif 'cat' in image:\n",
        "            y.append(0)\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd2flwegzW1A",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlEz3TgMD2bL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_extraction(X_train, X_test, y_train, y_test):\n",
        "  if metadata['competition_name']=='spooky-author-identification':\n",
        "    ngramLength = 5\n",
        "    print('fitting \"CountVectorizer()\" for bag of char %d-grams' %(ngramLength))\n",
        "    BagOfCharsExtractor = CountVectorizer(min_df=8, max_features=250000, \n",
        "                                          analyzer='char', ngram_range=(1,ngramLength), \n",
        "                                          binary=False,lowercase=True)\n",
        "    BagOfCharsExtractor.fit(pd.concat((X_train,X_test)))\n",
        "    X_train_char = BagOfCharsExtractor.transform(X_train)\n",
        "    X_valid_char = BagOfCharsExtractor.transform(X_test)\n",
        "    ngramLength = 2\n",
        "    print('fitting \"CountVectorizer()\" for bag of word %d-grams' %(ngramLength))\n",
        "    BagOfWordsExtractor = CountVectorizer(min_df=5, max_features=250000, \n",
        "                                          analyzer='word', ngram_range=(1,ngramLength), \n",
        "                                          binary=False,lowercase=True)\n",
        "    BagOfWordsExtractor.fit(pd.concat((X_train,X_test)))\n",
        "    X_train_word = BagOfWordsExtractor.transform(X_train)\n",
        "    X_valid_word = BagOfWordsExtractor.transform(X_test)\n",
        "    # combine and scale features \n",
        "    X_train = scipy.sparse.hstack((X_train_word,X_train_char))\n",
        "    X_test = scipy.sparse.hstack((X_valid_word,X_valid_char))\n",
        "    stdScaler = preproc.StandardScaler(with_mean=False)\n",
        "    stdScaler.fit(scipy.sparse.vstack(((X_train,X_test))))\n",
        "    X_train = stdScaler.transform(X_train)\n",
        "    X_test = stdScaler.transform(X_test)\n",
        " \n",
        "  else:\n",
        "    extractor = eval(metadata['feature_extractor'])\n",
        "    X_train = extractor.fit_transform(X_train, y_train)\n",
        "    X_test = extractor.fit_transform(X_test, y_test)\n",
        " \n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K3BYEqzzcPo",
        "colab_type": "text"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOtDPzvCD5YL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_selection(X_train, X_test, y_train, y_test):  \n",
        "  selector = eval(metadata['feature_selector'])\n",
        "  X_train = selector.fit_transform(X_train, y_train)\n",
        "  X_test = selector.fit_transform(X_test, y_test)\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pVGCiD6QFb3",
        "colab_type": "text"
      },
      "source": [
        "# **Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzqhUSdCQWB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for image augmentation\n",
        "def create_datagen():\n",
        "  datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "  return datagen\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tedJ6NDxQadI",
        "colab_type": "text"
      },
      "source": [
        "# **Neurual Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoiafwDOQj-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for CNN layer setting\n",
        "def CNN1(model):\n",
        "    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))\n",
        "    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\n",
        "    model.add(MaxPool2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation = \"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation = \"softmax\"))   \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-vK8zRwZlpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CNN2(model):\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.5))  #Dropout for regularization\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))  #Sigmoid function at the end because we have just two classes\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWUwBhxnMyT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gITHmtY5Qgr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t-lM6gfzhCx",
        "colab_type": "text"
      },
      "source": [
        "# Estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRx3WbLmQCdo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U32siwMD8WF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def estimation(X_train, X_test, y_train, y_test): \n",
        "  \n",
        "    start = time.time() \n",
        " \n",
        "    model = eval(metadata['estimator'])\n",
        "   \n",
        "  ######################### Keras&CNN: digit ######################### \n",
        "    if metadata['competition_name']=='digit-recognizer':   \n",
        "      CNN1(model)\n",
        "      # Define the optimizer\n",
        "      optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "      # Compile the model\n",
        "      model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "      # Set a learning rate annealer\n",
        "      learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)   \n",
        "      epochs = 10 # Turn epochs to 30 to get 0.9967 accuracy\n",
        "      batch_size = 86\n",
        "      # With data augmentation to prevent overfitting (accuracy 0.99286)#####################Using Data Augmentation\n",
        "      datagen = create_datagen()\n",
        "      datagen.fit(X_train)\n",
        "      # Fit the model\n",
        "      history = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (X_test,y_test),\n",
        "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
        "                              , callbacks=[learning_rate_reduction])  \n",
        "  ######################### Keras&CNN: digit #########################\n",
        "  \n",
        "  \n",
        "  ####################### Keras&CNN: Dog vs Cat ####################### \n",
        "    elif metadata['competition_name']=='dogs-vs-cats-redux-kernels-edition':\n",
        "      ntrain = len(X_train)\n",
        "      ntest = len(X_test)\n",
        "      batch_size = 32 \n",
        "      CNN2(model)\n",
        "      model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['acc'])\n",
        "      #hist=model.fit(X_train,y_train,epochs=64,batch_size=batch_size,validation_data=(X_test,y_test))\n",
        "  \n",
        "      #train_datagen,val_datagen=create_datagen2()\n",
        "      train_datagen=create_datagen()\n",
        "      train_datagen.fit(X_train)\n",
        "      #val_datagen.fit(X_test)\n",
        "      #train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
        "      #val_generator = val_datagen.flow(X_test, y_test, batch_size=batch_size)\n",
        "      history = model.fit_generator(train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "                              steps_per_epoch=ntrain // batch_size,\n",
        "                              epochs=64,\n",
        "                              validation_data=(X_test,y_test),\n",
        "                              validation_steps=ntest // batch_size)\n",
        "\n",
        "  ####################### Keras&CNN: Dog vs Cat #######################\n",
        "   \n",
        "    else:\n",
        "      model.fit(X_train, y_train)\n",
        "      predict = model.predict(X_test)\n",
        "      if metadata['metric'] == \"rmse\":  \n",
        "        error = np.sqrt(mean_squared_error(y_test, predict))\n",
        "      elif metadata['metric'] == \"accuracy\":\n",
        "        error = accuracy_score(y_test, predict)\n",
        "      elif metadata['metric'] == \"auc\":\n",
        "        fpr, tpr, _ = roc_curve(y_test, predict)\n",
        "        error = auc(fpr, tpr)\n",
        "      elif metadata['metric'] == \"logloss\":\n",
        "        proba = model.predict_proba(X_test)\n",
        "        error = log_loss(y_test, proba)\n",
        "      print(error)\n",
        "  \n",
        "    #print running time\n",
        "    end = time.time()   \n",
        "    print(\"Running time is:\"+str(end-start) + 's')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VstUncpAzsDv",
        "colab_type": "text"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRjIN5snGRN6",
        "colab_type": "text"
      },
      "source": [
        "Please refer to different training and testing dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMfNxyyyECzK",
        "colab_type": "code",
        "outputId": "6873a741-a8c8-492d-9e8a-3f33bb056dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "row_ids = [1,2,3,4]\n",
        "metadata={}\n",
        "\n",
        "#Set current working directory\n",
        "cwd = 'gdrive/My Drive/mlProject/'\n",
        "\n",
        "for row_id in row_ids:\n",
        "  metadata.clear()\n",
        "  print(\"************************************************************\")  \n",
        "  parseMetaData(row_id)\n",
        "  if metadata['competition_name']=='dogs-vs-cats-redux-kernels-edition':\n",
        "    train_df =cwd+metadata['competition_name'] + '/raw data'\n",
        "  else:  \n",
        "    competition_dir = cwd + metadata['competition_name'] + '/data/train.'+metadata['data_form']\n",
        "    #read data for different types\n",
        "    if metadata['data_form']=='csv':\n",
        "      train_df = pd.read_csv(competition_dir)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = preprocessing(train_df)\n",
        "  if metadata['feature_selector'].lower() != 'none':\n",
        "     X_train, X_test, y_train, y_test = feature_selection(X_train, X_test, y_train, y_test)\n",
        "  if metadata['feature_extractor']:\n",
        "     X_train, X_test, y_train, y_test = feature_extraction(X_train, X_test, y_train, y_test)    \n",
        "  estimation(X_train, X_test, y_train, y_test)\n",
        "  print(\"************************************************************\")\n",
        "#   X_train, X_test, y_train, y_test = preprocessing(train_df)\n",
        "#   if metadata['feature_selector'].lower() != 'none':\n",
        "#      X_train, X_test, y_train, y_test = feature_selection(X_train, X_test, y_train, y_test)\n",
        "#   if metadata['feature_extractor'].lower() !='none':\n",
        "#     X_train, X_test, y_train, y_test = feature_extraction(X_train, X_test, y_train, y_test)    \n",
        "#   estimation(X_train, X_test, y_train, y_test)\n",
        "#   print(\"************************************************************\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "************************************************************\n",
            "digit-recognizer\n",
            "['pixel0', 'pixel783']\n",
            "['label']\n",
            "[]\n",
            "Label\n",
            "accuracy\n",
            "none\n",
            "\n",
            "Sequential()\n",
            "(42000, 785)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "(102000, 785)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/30\n",
            " - 28s - loss: 0.2533 - acc: 0.9202 - val_loss: 0.0600 - val_acc: 0.9814\n",
            "Epoch 2/30\n",
            " - 24s - loss: 0.0822 - acc: 0.9757 - val_loss: 0.0328 - val_acc: 0.9898\n",
            "Epoch 3/30\n",
            " - 21s - loss: 0.0674 - acc: 0.9808 - val_loss: 0.0324 - val_acc: 0.9905\n",
            "Epoch 4/30\n",
            " - 22s - loss: 0.0652 - acc: 0.9815 - val_loss: 0.0267 - val_acc: 0.9918\n",
            "Epoch 5/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-aecbdeaa2118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_extractor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m      \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mestimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"************************************************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#   X_train, X_test, y_train, y_test = preprocessing(train_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-f0dbcabf087a>\u001b[0m in \u001b[0;36mestimation\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     23\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                               \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                               , callbacks=[learning_rate_reduction])  \n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0;31m######################### Keras&CNN: digit #########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhQK7oR56UDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}